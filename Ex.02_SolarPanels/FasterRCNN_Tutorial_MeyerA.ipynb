{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FasterRCNN_Tutorial_MeyerA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBG7ZQ_pcEah"
      },
      "source": [
        "# Object Detection Tutorial with Faster R-CNN Transfer Learning\n",
        "## Geopython 2019, Adrian F. Meyer\n",
        "Parts of this tutorial are based on the [Medium article](https://hackernoon.com/object-detection-in-google-colab-with-custom-dataset-5a7bb2b0e97e) by RomRoc, 2018.\n",
        "\n",
        "**In this Tutorial we will learn, how to use the Tensorflow Object Detection library, to detect solar panels on tiles of an aerial orthomosaic.**\n",
        "\n",
        "![alt text](https://wilddrone.ch/wp-content/uploads/2019/06/solardetector.jpg)\n",
        "\n",
        "The code, libraries and cloud environments used in this tutorial are currently available for free and are generally released open source.\n",
        "You will need a Google account to execute the Notebook in its entirety, because it is meant to be executed on the Google Colab platform.\n",
        "\n",
        "*Go to [Google Colab](https://colab.research.google.com/) and upload the notebook there. Make sure that you use Python 3 and GPU hardware acceleration as Runtime Environment.*\n",
        "\n",
        "The dataset provided is based on the publically available SwissImage orthomosaic by [SwissTopo](https://map.geo.admin.ch/?topic=swisstopo&lang=de&bgLayer=ch.swisstopo.swissimage). \n",
        "The images and annotations can be downloaded as Zip File (31 Mbyte) here:\n",
        "[https://drive.google.com/file/d/1i9RlEJTeB-KRauwhuMp-Vl13-Z3Yrj_y/view?usp=sharing](https://drive.google.com/file/d/1i9RlEJTeB-KRauwhuMp-Vl13-Z3Yrj_y/view?usp=sharing)\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://wilddrone.ch/wp-content/uploads/2019/06/Tiles.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWJHCmFs_Vg6"
      },
      "source": [
        "# Download Tensorflow Repo and Python Modules\n",
        "By executing the first code snippet you initialize your virtual linux-style machine. Use The little arrow \">\" in the top left corner to view the file system of your hosted system.\n",
        "You can use UNIX-style terminal commands by using the prefix % and elevated priviledge commands for installations with the prefix !."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsxgHEoV8J56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "f9ca9c68-6c80-469c-ca0a-c96e5ba916ff"
      },
      "source": [
        "#make sure numpy is downgraded for compatibility reasons. \n",
        "#It will throw an error, which is not a problem, just click on the newly\n",
        "#appeared Button \"Restart Runtime\"\n",
        "!pip install numpy==1.17.4"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.17.4\n",
            "  Downloading numpy-1.17.4-cp37-cp37m-manylinux1_x86_64.whl (20.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.0 MB 7.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.17.4 which is incompatible.\n",
            "kapre 0.3.5 requires numpy>=1.18.5, but you have numpy 1.17.4 which is incompatible.\n",
            "jaxlib 0.1.71+cuda111 requires numpy>=1.18, but you have numpy 1.17.4 which is incompatible.\n",
            "jax 0.2.21 requires numpy>=1.18, but you have numpy 1.17.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.17.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9NdpuUj8MB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06bf5838-86be-4316-d7d9-5411fbca4f07"
      },
      "source": [
        "%cd /content\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "#make sure to be in /root and that tensorflow is running in version 1.15.2\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ7NlRzJ8OKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c35cc9-c84d-4065-8a16-3fa372c0ab00"
      },
      "source": [
        "\"\"\"\n",
        "This allows you to check which GPU you have been allocated. Google offers free\n",
        "Tesla T4, Tesla K80, Tesla P100 (the P100 hax 1.6x more GFLOPs and 3x the memory bandwith than K80, the T4 is fairly slow).\n",
        "In theory you can restart the environment until you have the fast one. \n",
        "For testing and learning it doesn't really matter.\n",
        "\"\"\"\n",
        "  \n",
        "#We have to work with Tensorflow 1.15.2 for code compatibility reasons; by now TF v2 is available.\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Oct 10 13:20:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    58W / 149W |     60MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oGeQyAVvB7F",
        "outputId": "1afd343d-fa0e-4065-f917-0e799be987aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content\n",
        "  \n",
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 64364, done.\u001b[K\n",
            "remote: Total 64364 (delta 0), reused 0 (delta 0), pack-reused 64364\u001b[K\n",
            "Receiving objects: 100% (64364/64364), 575.04 MiB | 29.62 MiB/s, done.\n",
            "Resolving deltas: 100% (45043/45043), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dyvc3xTxsy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226628a4-69ef-48fb-d368-74eccbfcc31d"
      },
      "source": [
        "%cd /content\n",
        "  \n",
        "\"\"\"\n",
        "This repository contains a number of different models implemented in TensorFlow:\n",
        "The official models are a collection of example models that use TensorFlow's high-level APIs. They are intended to be well-maintained, tested, and kept up to date with the latest stable TensorFlow API. They should also be reasonably optimized for fast performance while still being easy to read. We especially recommend newer TensorFlow users to start here.\n",
        "The research models are a large collection of models implemented in TensorFlow by researchers. They are not officially supported or available in release branches; it is up to the individual researchers to maintain the models and/or provide support on issues and pull requests.\n",
        "The samples folder contains code snippets and smaller models that demonstrate features of TensorFlow, including code presented in various blog posts.\n",
        "\"\"\"  \n",
        "\n",
        "!apt-get install protobuf-compiler python-tk\n",
        "\n",
        "\"\"\"\n",
        "Protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data; similar to JSON or XML.\n",
        "\"\"\"\n",
        "\n",
        "!pip install Cython contextlib2 pillow lxml matplotlib PyDrive\n",
        "\"\"\"\n",
        "These context modules are necessary python pachages. Especially Cython is important: It allows to call native C or C++ bindings from within python.\n",
        "\"\"\"\n",
        "\n",
        "!pip install pycocotools\n",
        "\"\"\"\n",
        "COCO is a large image dataset designed for object detection, segmentation, person keypoints detection, stuff segmentation, and caption generation. \n",
        "\"\"\"\n",
        "\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=. #This initializes/compiles the Tensorflow Protobuf evnironment.\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/models/research/:/models/research/slim/'\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "protobuf-compiler is already the newest version (3.0.0-9.1ubuntu1).\n",
            "python-tk is already the newest version (2.7.17-1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.24)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (0.5.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.17.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (1.12.8)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.26.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (57.4.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (21.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.53.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.7.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.24.3)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools) (57.4.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools) (0.29.24)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.17.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.15.0)\n",
            "/content/models/research\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVXThN8P_itE"
      },
      "source": [
        "# Install Tensorflow on Virtual Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alxt_PF4yBae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "150a3134-7f12-4595-f905-3ea77999a8f2"
      },
      "source": [
        "%cd /content/models/research/slim\n",
        "!python setup.py build\n",
        "!python setup.py install > /dev/null\n",
        "\"\"\"\n",
        "This snippet builds and installs the Tensorflow API from the cloned git source.\n",
        "\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/slim\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/preprocessing\n",
            "copying preprocessing/preprocessing_factory.py -> build/lib/preprocessing\n",
            "copying preprocessing/__init__.py -> build/lib/preprocessing\n",
            "copying preprocessing/lenet_preprocessing.py -> build/lib/preprocessing\n",
            "copying preprocessing/inception_preprocessing.py -> build/lib/preprocessing\n",
            "copying preprocessing/vgg_preprocessing.py -> build/lib/preprocessing\n",
            "copying preprocessing/cifarnet_preprocessing.py -> build/lib/preprocessing\n",
            "creating build/lib/nets\n",
            "copying nets/i3d_test.py -> build/lib/nets\n",
            "copying nets/inception_v2_test.py -> build/lib/nets\n",
            "copying nets/s3dg.py -> build/lib/nets\n",
            "copying nets/i3d.py -> build/lib/nets\n",
            "copying nets/alexnet_test.py -> build/lib/nets\n",
            "copying nets/i3d_utils.py -> build/lib/nets\n",
            "copying nets/dcgan.py -> build/lib/nets\n",
            "copying nets/inception_v1.py -> build/lib/nets\n",
            "copying nets/__init__.py -> build/lib/nets\n",
            "copying nets/mobilenet_v1_eval.py -> build/lib/nets\n",
            "copying nets/inception.py -> build/lib/nets\n",
            "copying nets/dcgan_test.py -> build/lib/nets\n",
            "copying nets/inception_v3_test.py -> build/lib/nets\n",
            "copying nets/inception_v2.py -> build/lib/nets\n",
            "copying nets/mobilenet_v1_train.py -> build/lib/nets\n",
            "copying nets/inception_resnet_v2.py -> build/lib/nets\n",
            "copying nets/mobilenet_v1_test.py -> build/lib/nets\n",
            "copying nets/overfeat.py -> build/lib/nets\n",
            "copying nets/s3dg_test.py -> build/lib/nets\n",
            "copying nets/vgg_test.py -> build/lib/nets\n",
            "copying nets/inception_v3.py -> build/lib/nets\n",
            "copying nets/inception_resnet_v2_test.py -> build/lib/nets\n",
            "copying nets/inception_utils.py -> build/lib/nets\n",
            "copying nets/resnet_v2.py -> build/lib/nets\n",
            "copying nets/alexnet.py -> build/lib/nets\n",
            "copying nets/overfeat_test.py -> build/lib/nets\n",
            "copying nets/post_training_quantization.py -> build/lib/nets\n",
            "copying nets/cyclegan.py -> build/lib/nets\n",
            "copying nets/lenet.py -> build/lib/nets\n",
            "copying nets/inception_v4_test.py -> build/lib/nets\n",
            "copying nets/inception_v4.py -> build/lib/nets\n",
            "copying nets/pix2pix_test.py -> build/lib/nets\n",
            "copying nets/resnet_utils.py -> build/lib/nets\n",
            "copying nets/vgg.py -> build/lib/nets\n",
            "copying nets/nets_factory_test.py -> build/lib/nets\n",
            "copying nets/inception_v1_test.py -> build/lib/nets\n",
            "copying nets/cyclegan_test.py -> build/lib/nets\n",
            "copying nets/resnet_v1.py -> build/lib/nets\n",
            "copying nets/resnet_v2_test.py -> build/lib/nets\n",
            "copying nets/nets_factory.py -> build/lib/nets\n",
            "copying nets/pix2pix.py -> build/lib/nets\n",
            "copying nets/resnet_v1_test.py -> build/lib/nets\n",
            "copying nets/mobilenet_v1.py -> build/lib/nets\n",
            "copying nets/cifarnet.py -> build/lib/nets\n",
            "creating build/lib/deployment\n",
            "copying deployment/model_deploy_test.py -> build/lib/deployment\n",
            "copying deployment/__init__.py -> build/lib/deployment\n",
            "copying deployment/model_deploy.py -> build/lib/deployment\n",
            "creating build/lib/datasets\n",
            "copying datasets/flowers.py -> build/lib/datasets\n",
            "copying datasets/dataset_utils.py -> build/lib/datasets\n",
            "copying datasets/cifar10.py -> build/lib/datasets\n",
            "copying datasets/build_imagenet_data.py -> build/lib/datasets\n",
            "copying datasets/imagenet.py -> build/lib/datasets\n",
            "copying datasets/__init__.py -> build/lib/datasets\n",
            "copying datasets/download_and_convert_flowers.py -> build/lib/datasets\n",
            "copying datasets/download_and_convert_cifar10.py -> build/lib/datasets\n",
            "copying datasets/process_bounding_boxes.py -> build/lib/datasets\n",
            "copying datasets/visualwakewords.py -> build/lib/datasets\n",
            "copying datasets/download_and_convert_mnist.py -> build/lib/datasets\n",
            "copying datasets/dataset_factory.py -> build/lib/datasets\n",
            "copying datasets/preprocess_imagenet_validation_data.py -> build/lib/datasets\n",
            "copying datasets/mnist.py -> build/lib/datasets\n",
            "copying datasets/download_and_convert_visualwakewords_lib.py -> build/lib/datasets\n",
            "copying datasets/download_and_convert_visualwakewords.py -> build/lib/datasets\n",
            "creating build/lib/nets/nasnet\n",
            "copying nets/nasnet/pnasnet_test.py -> build/lib/nets/nasnet\n",
            "copying nets/nasnet/__init__.py -> build/lib/nets/nasnet\n",
            "copying nets/nasnet/nasnet_utils_test.py -> build/lib/nets/nasnet\n",
            "copying nets/nasnet/pnasnet.py -> build/lib/nets/nasnet\n",
            "copying nets/nasnet/nasnet_test.py -> build/lib/nets/nasnet\n",
            "copying nets/nasnet/nasnet_utils.py -> build/lib/nets/nasnet\n",
            "copying nets/nasnet/nasnet.py -> build/lib/nets/nasnet\n",
            "creating build/lib/nets/mobilenet\n",
            "copying nets/mobilenet/conv_blocks.py -> build/lib/nets/mobilenet\n",
            "copying nets/mobilenet/__init__.py -> build/lib/nets/mobilenet\n",
            "copying nets/mobilenet/mobilenet_v3.py -> build/lib/nets/mobilenet\n",
            "copying nets/mobilenet/mobilenet_v3_test.py -> build/lib/nets/mobilenet\n",
            "copying nets/mobilenet/mobilenet_v2.py -> build/lib/nets/mobilenet\n",
            "copying nets/mobilenet/mobilenet.py -> build/lib/nets/mobilenet\n",
            "copying nets/mobilenet/mobilenet_v2_test.py -> build/lib/nets/mobilenet\n",
            "running egg_info\n",
            "creating slim.egg-info\n",
            "writing slim.egg-info/PKG-INFO\n",
            "writing dependency_links to slim.egg-info/dependency_links.txt\n",
            "writing requirements to slim.egg-info/requires.txt\n",
            "writing top-level names to slim.egg-info/top_level.txt\n",
            "writing manifest file 'slim.egg-info/SOURCES.txt'\n",
            "writing manifest file 'slim.egg-info/SOURCES.txt'\n",
            "copying nets/mobilenet_v1.md -> build/lib/nets\n",
            "copying nets/mobilenet_v1.png -> build/lib/nets\n",
            "copying datasets/download_and_convert_imagenet.sh -> build/lib/datasets\n",
            "copying datasets/download_imagenet.sh -> build/lib/datasets\n",
            "copying datasets/imagenet_2012_validation_synset_labels.txt -> build/lib/datasets\n",
            "copying datasets/imagenet_lsvrc_2015_synsets.txt -> build/lib/datasets\n",
            "copying datasets/imagenet_metadata.txt -> build/lib/datasets\n",
            "copying nets/nasnet/README.md -> build/lib/nets/nasnet\n",
            "copying nets/mobilenet/README.md -> build/lib/nets/mobilenet\n",
            "copying nets/mobilenet/mnet_v1_vs_v2_pixel1_latency.png -> build/lib/nets/mobilenet\n",
            "copying nets/mobilenet/mobilenet_example.ipynb -> build/lib/nets/mobilenet\n",
            "creating build/lib/nets/mobilenet/g3doc\n",
            "copying nets/mobilenet/g3doc/edgetpu_latency.png -> build/lib/nets/mobilenet/g3doc\n",
            "copying nets/mobilenet/g3doc/latency_pixel1.png -> build/lib/nets/mobilenet/g3doc\n",
            "copying nets/mobilenet/g3doc/madds_top1_accuracy.png -> build/lib/nets/mobilenet/g3doc\n",
            "zip_safe flag not set; analyzing archive contents...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThis snippet builds and installs the Tensorflow API from the cloned git source.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr4Qf_v-SDeQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THX1UucO0L5X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "172577dd-a6a2-4349-880b-881dbb0fa411"
      },
      "source": [
        "%cd slim\n",
        "!pip install -e .\n",
        "\n",
        "%cd ..\n",
        "!python object_detection/builders/model_builder_test.py\n",
        "\"\"\"\n",
        "This tests if the installation was successful. The Tests should yield the output [ RUN  OK ]\n",
        "\"\"\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'slim'\n",
            "/content/models/research/slim\n",
            "Obtaining file:///content/models/research/slim\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from slim==0.1) (1.15.0)\n",
            "Requirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.7/dist-packages/tf_slim-1.1.0-py3.7.egg (from slim==0.1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf-slim>=1.1->slim==0.1) (0.12.0)\n",
            "Installing collected packages: slim\n",
            "  Attempting uninstall: slim\n",
            "    Found existing installation: slim 0.1\n",
            "    Uninstalling slim-0.1:\n",
            "      Successfully uninstalled slim-0.1\n",
            "  Running setup.py develop for slim\n",
            "Successfully installed slim-0.1\n",
            "/content/models/research\n",
            "Traceback (most recent call last):\n",
            "  File \"object_detection/builders/model_builder_test.py\", line 21, in <module>\n",
            "    from object_detection.builders import model_builder\n",
            "ModuleNotFoundError: No module named 'object_detection'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThis tests if the installation was successful. The Tests should yield the output [ RUN  OK ]\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceHKJMfzHdg9"
      },
      "source": [
        "#Upload and Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI9EJ13NBLVd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "139fecef-157e-4a5c-9882-35ddd9232cbc"
      },
      "source": [
        "%cd /content/drive/MyDrive\n",
        "!wget http://2019.geopython.net/data/solar.zip\n",
        "\"\"\"\n",
        "We download the images, annotation files and independent test samples into the datalab folder.\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/datalab\n",
            "--2021-10-10 10:21:10--  http://2019.geopython.net/data/solar.zip\n",
            "Resolving 2019.geopython.net (2019.geopython.net)... 147.86.7.67\n",
            "Connecting to 2019.geopython.net (2019.geopython.net)|147.86.7.67|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33438193 (32M) [application/zip]\n",
            "Saving to: ‘solar.zip’\n",
            "\n",
            "solar.zip           100%[===================>]  31.89M  74.8MB/s    in 0.4s    \n",
            "\n",
            "2021-10-10 10:21:11 (74.8 MB/s) - ‘solar.zip’ saved [33438193/33438193]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nWe download the images, annotation files and independent test samples into the datalab folder.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaaI150MDVLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "745815ed-9abe-4f1c-d7fc-1c3f0d09cdd8"
      },
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "!unzip main.zip #Scroll through the unzip output to get an idea of the datalab folder content."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "Archive:  main.zip\n",
            "   creating: main/\n",
            "   creating: main/annotations/\n",
            "   creating: main/annotations/xmls/\n",
            "  inflating: main/annotations/xmls/frame (1).xml  \n",
            "  inflating: main/annotations/xmls/frame (10).xml  \n",
            "  inflating: main/annotations/xmls/frame (11).xml  \n",
            "  inflating: main/annotations/xmls/frame (12).xml  \n",
            "  inflating: main/annotations/xmls/frame (13).xml  \n",
            "  inflating: main/annotations/xmls/frame (14).xml  \n",
            "  inflating: main/annotations/xmls/frame (15).xml  \n",
            "  inflating: main/annotations/xmls/frame (16).xml  \n",
            "  inflating: main/annotations/xmls/frame (2).xml  \n",
            "  inflating: main/annotations/xmls/frame (3).xml  \n",
            "  inflating: main/annotations/xmls/frame (4).xml  \n",
            "  inflating: main/annotations/xmls/frame (5).xml  \n",
            "  inflating: main/annotations/xmls/frame (6).xml  \n",
            "  inflating: main/annotations/xmls/frame (7).xml  \n",
            "  inflating: main/annotations/xmls/frame (8).xml  \n",
            "  inflating: main/annotations/xmls/frame (9).xml  \n",
            "  inflating: main/annotations/xmls/framee (1).xml  \n",
            "  inflating: main/annotations/xmls/framee (10).xml  \n",
            "  inflating: main/annotations/xmls/framee (11).xml  \n",
            "  inflating: main/annotations/xmls/framee (12).xml  \n",
            "  inflating: main/annotations/xmls/framee (13).xml  \n",
            "  inflating: main/annotations/xmls/framee (14).xml  \n",
            "  inflating: main/annotations/xmls/framee (15).xml  \n",
            "  inflating: main/annotations/xmls/framee (16).xml  \n",
            "  inflating: main/annotations/xmls/framee (2).xml  \n",
            "  inflating: main/annotations/xmls/framee (21).xml  \n",
            "  inflating: main/annotations/xmls/framee (22).xml  \n",
            "  inflating: main/annotations/xmls/framee (23).xml  \n",
            "  inflating: main/annotations/xmls/framee (24).xml  \n",
            "  inflating: main/annotations/xmls/framee (25).xml  \n",
            "  inflating: main/annotations/xmls/framee (26).xml  \n",
            "  inflating: main/annotations/xmls/framee (27).xml  \n",
            "  inflating: main/annotations/xmls/framee (28).xml  \n",
            "  inflating: main/annotations/xmls/framee (29).xml  \n",
            "  inflating: main/annotations/xmls/framee (3).xml  \n",
            "  inflating: main/annotations/xmls/framee (30).xml  \n",
            "  inflating: main/annotations/xmls/framee (31).xml  \n",
            "  inflating: main/annotations/xmls/framee (32).xml  \n",
            "  inflating: main/annotations/xmls/framee (33).xml  \n",
            "  inflating: main/annotations/xmls/framee (34).xml  \n",
            "  inflating: main/annotations/xmls/framee (35).xml  \n",
            "  inflating: main/annotations/xmls/framee (36).xml  \n",
            "  inflating: main/annotations/xmls/framee (37).xml  \n",
            "  inflating: main/annotations/xmls/framee (38).xml  \n",
            "  inflating: main/annotations/xmls/framee (39).xml  \n",
            "  inflating: main/annotations/xmls/framee (4).xml  \n",
            "  inflating: main/annotations/xmls/framee (40).xml  \n",
            "  inflating: main/annotations/xmls/framee (5).xml  \n",
            "  inflating: main/annotations/xmls/framee (6).xml  \n",
            "  inflating: main/annotations/xmls/framee (7).xml  \n",
            "  inflating: main/annotations/xmls/framee (8).xml  \n",
            "  inflating: main/annotations/xmls/framee (9).xml  \n",
            "   creating: main/images/\n",
            "  inflating: main/images/frame (1).jpg  \n",
            "  inflating: main/images/frame (10).jpg  \n",
            "  inflating: main/images/frame (11).jpg  \n",
            "  inflating: main/images/frame (12).jpg  \n",
            "  inflating: main/images/frame (13).jpg  \n",
            "  inflating: main/images/frame (14).jpg  \n",
            "  inflating: main/images/frame (15).jpg  \n",
            "  inflating: main/images/frame (16).jpg  \n",
            "  inflating: main/images/frame (2).jpg  \n",
            "  inflating: main/images/frame (3).jpg  \n",
            "  inflating: main/images/frame (4).jpg  \n",
            "  inflating: main/images/frame (5).jpg  \n",
            "  inflating: main/images/frame (6).jpg  \n",
            "  inflating: main/images/frame (7).jpg  \n",
            "  inflating: main/images/frame (8).jpg  \n",
            "  inflating: main/images/frame (9).jpg  \n",
            "  inflating: main/images/framee (1).jpg  \n",
            "  inflating: main/images/framee (10).jpg  \n",
            "  inflating: main/images/framee (11).jpg  \n",
            "  inflating: main/images/framee (12).jpg  \n",
            "  inflating: main/images/framee (13).jpg  \n",
            "  inflating: main/images/framee (14).jpg  \n",
            "  inflating: main/images/framee (15).jpg  \n",
            "  inflating: main/images/framee (16).jpg  \n",
            "  inflating: main/images/framee (2).jpg  \n",
            "  inflating: main/images/framee (21).jpg  \n",
            "  inflating: main/images/framee (22).jpg  \n",
            "  inflating: main/images/framee (23).jpg  \n",
            "  inflating: main/images/framee (24).jpg  \n",
            "  inflating: main/images/framee (25).jpg  \n",
            "  inflating: main/images/framee (26).jpg  \n",
            "  inflating: main/images/framee (27).jpg  \n",
            "  inflating: main/images/framee (28).jpg  \n",
            "  inflating: main/images/framee (29).jpg  \n",
            "  inflating: main/images/framee (3).jpg  \n",
            "  inflating: main/images/framee (30).jpg  \n",
            "  inflating: main/images/framee (31).jpg  \n",
            "  inflating: main/images/framee (32).jpg  \n",
            "  inflating: main/images/framee (33).jpg  \n",
            "  inflating: main/images/framee (34).jpg  \n",
            "  inflating: main/images/framee (35).jpg  \n",
            "  inflating: main/images/framee (36).jpg  \n",
            "  inflating: main/images/framee (37).jpg  \n",
            "  inflating: main/images/framee (38).jpg  \n",
            "  inflating: main/images/framee (39).jpg  \n",
            "  inflating: main/images/framee (4).jpg  \n",
            "  inflating: main/images/framee (40).jpg  \n",
            "  inflating: main/images/framee (5).jpg  \n",
            "  inflating: main/images/framee (6).jpg  \n",
            "  inflating: main/images/framee (7).jpg  \n",
            "  inflating: main/images/framee (8).jpg  \n",
            "  inflating: main/images/framee (9).jpg  \n",
            "   creating: main/testsamples/\n",
            "  inflating: main/testsamples/framee (18).jpg  \n",
            "  inflating: main/testsamples/framee (19).jpg  \n",
            "  inflating: main/testsamples/framee (20).jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LMyNhmeS-Yc"
      },
      "source": [
        "You can download one of the XML files to check the structure of the PASCAL VOC Annotation format.\n",
        "Here is an example:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "<annotation>\n",
        "\t<folder>sol2</folder>\n",
        "\t<filename>solar_10.JPG</filename>\n",
        "\t<path>C:\\temp\\sol2\\solar_10.JPG</path>\n",
        "\t<source>\n",
        "\t\t<database>Unknown</database>\n",
        "\t</source>\n",
        "\t<size>\n",
        "\t\t<width>901</width>\n",
        "\t\t<height>791</height>\n",
        "\t\t<depth>3</depth>\n",
        "\t</size>\n",
        "\t<segmented>0</segmented>\n",
        "\t<object>\n",
        "\t\t<name>solar</name>\n",
        "\t\t<pose>Unspecified</pose>\n",
        "\t\t<truncated>1</truncated>\n",
        "\t\t<difficult>0</difficult>\n",
        "\t\t<bndbox>\n",
        "\t\t\t<xmin>637</xmin>\n",
        "\t\t\t<ymin>152</ymin>\n",
        "\t\t\t<xmax>901</xmax>\n",
        "\t\t\t<ymax>591</ymax>\n",
        "\t\t</bndbox>\n",
        "\t</object>\n",
        "</annotation>\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osh2Ty-vTejr"
      },
      "source": [
        "We need to write our label name (in this case \"solar\") into a config file defining all detectable classes.\n",
        "In our case it is just on class.\n",
        "\n",
        "Then we iterate through all image files to extract the file names (paths are not relevant) we want to use for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f41p5Nb1Uhl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e12ae72-e012-4910-d8f7-d85a0a05d389"
      },
      "source": [
        "%cd ..\n",
        "%cd /content/drive/MyDrive/main\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models\n",
            "/content/drive/MyDrive/main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2anYUa28Ylpy"
      },
      "source": [
        "!echo \"item { id: 1 name: 'True'}\" > label_map.pbtxt\n",
        "\n",
        "image_files=os.listdir('images')\n",
        "im_files=[x.split('.')[0] for x in image_files]\n",
        "with open('annotations/trainval.txt', 'w') as text_file:\n",
        "  for row in im_files:\n",
        "    text_file.write(row + '\\n')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiE8dMrBMGTG"
      },
      "source": [
        "#Data and Model Preparation\n",
        "## Generate Bounding Boxes on Images for RPN Network Training\n",
        "The same process need to be performed with the XML Annotation files.\n",
        "Additionally, we convert the images to PNG format for easier access."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r38LiZJ11nJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c37a2261-2719-469e-dc5a-1491746d4a75"
      },
      "source": [
        "%cd /content/drive/MyDrive/main/annotations\n",
        "\n",
        "!mkdir trimaps\n",
        "\n",
        "from PIL import Image\n",
        "image = Image.new('RGB', (901, 791))\n",
        "\n",
        "for filename in os.listdir('xmls'):\n",
        "  filename = os.path.splitext(filename)[0]\n",
        "  image.save('trimaps/' + filename + '.png')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/main/annotations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlG7LCylMYB_"
      },
      "source": [
        "##Generate Labelled Tensor Matrices (tf_records)\n",
        "The Tensorflow Record files contain the actual input data for the Machine Learning process in binary format.\n",
        "An API specific script can do the job for us.\n",
        "We use the famous \"pet faces model\" in our transfer learning process.\n",
        "We need to split the dataset at this point into training and validation data.\n",
        "70% of our data (148 of 212 images) will be used for training, the remaining 30% for validation (64 images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhwFAOny2OLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c38b60a4-9285-4c59-f8f1-671e2b8f1c29"
      },
      "source": [
        "#%cd /content/drive/MyDrive/main\\\n",
        "%cd /content/models/research/slim\n",
        "\n",
        "!python /content/models/research/object_detection/dataset_tools/create_pet_tf_record.py --label_map_path=label_map.pbtxt --data_dir=. --output_dir=. --num_shards=1\n",
        "\n",
        "!mv pet_faces_train.record-00000-of-00001 tf_train.record\n",
        "\n",
        "!mv pet_faces_val.record-00000-of-00001 tf_val.record"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/slim\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/models/research/object_detection/dataset_tools/create_pet_tf_record.py\", line 42, in <module>\n",
            "    from object_detection.dataset_tools import tf_record_creation_util\n",
            "ModuleNotFoundError: No module named 'object_detection'\n",
            "mv: cannot stat 'pet_faces_train.record-00000-of-00001': No such file or directory\n",
            "mv: cannot stat 'pet_faces_val.record-00000-of-00001': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0CoGNKoMiCr"
      },
      "source": [
        "##Download the Model Checkpoint you want to use for Transfer Learning\n",
        "Many different COCO pretrained neural models can be used for bounding box related object detection with Tensorflow.\n",
        "They all have different advantages or disadvantages (e.g. inferencing speed, accuracy, easy to train, etc.).\n",
        "\n",
        "An overview can be found with the [TF Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Sj2b08bHfmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d88cfe-9356-4750-c8cc-3d12a9a7ebc3"
      },
      "source": [
        "%cd /content/drive/MyDrive/main\n",
        "!wget http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz\n",
        "  \n",
        "%cd /content/drive/MyDrive/main\n",
        "!tar -xvzf faster_rcnn_inception_v2_coco_2018_01_28.tar.gz\n",
        "\n",
        "%cd /content/drive/MyDrive/main\n",
        "!mv faster_rcnn_inception_v2_coco_2018_01_28 pretrained_model"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/main\n",
            "--2021-10-10 13:29:15--  http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 142.250.125.128, 2607:f8b0:4001:c16::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|142.250.125.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149119618 (142M) [application/x-tar]\n",
            "Saving to: ‘faster_rcnn_inception_v2_coco_2018_01_28.tar.gz’\n",
            "\n",
            "faster_rcnn_incepti 100%[===================>] 142.21M  45.5MB/s    in 3.1s    \n",
            "\n",
            "2021-10-10 13:29:18 (45.5 MB/s) - ‘faster_rcnn_inception_v2_coco_2018_01_28.tar.gz’ saved [149119618/149119618]\n",
            "\n",
            "/content/drive/MyDrive/main\n",
            "faster_rcnn_inception_v2_coco_2018_01_28/\n",
            "faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt.index\n",
            "faster_rcnn_inception_v2_coco_2018_01_28/checkpoint\n",
            "faster_rcnn_inception_v2_coco_2018_01_28/pipeline.config\n",
            "faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt.data-00000-of-00001\n",
            "faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt.meta\n",
            "faster_rcnn_inception_v2_coco_2018_01_28/saved_model/\n",
            "faster_rcnn_inception_v2_coco_2018_01_28/saved_model/saved_model.pb\n",
            "faster_rcnn_inception_v2_coco_2018_01_28/saved_model/variables/\n",
            "faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb\n",
            "/content/drive/MyDrive/main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyk6YNdBMq8A"
      },
      "source": [
        "##Configure the Paths and Training Parameters\n",
        "This specifies which files and model checkpoints should be used for the trainings process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkNT0AWT6Q9K"
      },
      "source": [
        "#%cd \\content\\\n",
        "\n",
        "import re\n",
        "\n",
        "#filename = '/datalab/pretrained_model/pipeline.config'\n",
        "filename = '/content/models/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config'\n",
        "with open(filename) as f:\n",
        "  s = f.read()\n",
        "with open(filename, 'w') as f:\n",
        "  s = re.sub('PATH_TO_BE_CONFIGURED/model.ckpt', '/content/drive/MyDrive/main/pretrained_model/model.ckpt', s)\n",
        "  s = re.sub('PATH_TO_BE_CONFIGURED/pet_faces_train.record-\\?\\?\\?\\?\\?-of-00010', '/content/drive/MyDrive/main/tf_train.record', s)\n",
        "  s = re.sub('PATH_TO_BE_CONFIGURED/pet_faces_val.record-\\?\\?\\?\\?\\?-of-00010', '/content/drive/MyDrive/main/tf_test.record', s)\n",
        "  s = re.sub('PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt', '/content/drive/MyDrive/main/label_map.pbtxt', s)\n",
        "  f.write(s)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiJ6kHszMyeJ"
      },
      "source": [
        "# Training on GPU\n",
        "The execution of this snippet might take a while. Normally the training of 3000 steps should take about 10 minutes (approx. 12 seconds for 100 steps). \n",
        "\n",
        "As a rough estimate, the loss value of Faster RCNN models should fall below 0.05 over a few thousand steps and then the training can be aborted. \n",
        "\n",
        "We configure automatic termination after 3'000 Steps, in productive trainings as much as 100'000-200'000 Steps can be neccesary.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*qGb5XNny5G8PrGQ5sejFvw.png)\n",
        "\n",
        "This graph shows the expected training progess of the model with the supervision tool \"Tensorboard\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mr7BM3Yb_jL",
        "outputId": "8ea5e2eb-d7df-4792-b489-9c87230438f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/models/research/object_detection\n",
        "!pip install object_detection"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement object_detection (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for object_detection\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvGQxjNI6QNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9982d230-1a2b-4215-cd88-737b1461871b"
      },
      "source": [
        "#%cd /content/drive/MyDrive/main\n",
        "\n",
        "#%cp -R /content\n",
        "\n",
        "!python /content/models/research/object_detection/model_main.py\\\n",
        "    --pipeline_config_path=/content/models/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config \\\n",
        "    --model_dir=/content/drive/MyDrive/main/trained \\\n",
        "    --train_dir=/content/drive/MyDrive/main/trained \\\n",
        "    --logtostderr \\\n",
        "    --logdir=/content/drive/MyDrive/main/trained \\\n",
        "    --num_train_steps=3000 \\\n",
        "    --num_eval_steps=500 \\\n",
        "    --max_evals=0 "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/models/research/object_detection/model_main.py\", line 25, in <module>\n",
            "    from object_detection import model_lib\n",
            "ModuleNotFoundError: No module named 'object_detection'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6aIMnXJOm49"
      },
      "source": [
        "# Export Inference Graph\n",
        "Inferencing means to apply the model to images which haven't been used for training.\n",
        "\n",
        "We reserved a few images to check if our model performs correctly.\n",
        "\n",
        "The frozen Inference Graph gets generated from the last model checkpoint and contains all elements of the model neccesary to perform inference (also on weaker hardware), but it cannot be used to continue training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsjkGdgTDmbh"
      },
      "source": [
        "%cd /datalab\n",
        "\n",
        "lst = os.listdir('trained')\n",
        "lf = filter(lambda k: 'model.ckpt-' in k, lst)\n",
        "last_model = sorted(lf)[-1].replace('.meta', '')\n",
        "\n",
        "!python ~/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path=/root/models/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config \\\n",
        "    --output_directory=fine_tuned_model \\\n",
        "    --trained_checkpoint_prefix=trained/$last_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQub8i2wareJ"
      },
      "source": [
        "##Alternative: Import a Finished Inference Graph\n",
        "Uncomment this section, if you want to download a finished inference graph if you could not finish the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9Emd73ISTk9"
      },
      "source": [
        "#Alternative to training, you can download a fully trained model inference graph by uncommenting (#) the following lines\n",
        "\n",
        "#%cd /datalab\n",
        "#!wget http://2019.geopython.net/data/trained.tar.gz\n",
        "#%cd /datalab\n",
        "#!tar -xzf trained.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqAjy3fpOzlZ"
      },
      "source": [
        "# Run Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tO_OCUXEvjr"
      },
      "source": [
        "%cd /root/models/research/object_detection\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "#if tf.__version__ < '1.4.0':\n",
        "#  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from utils import label_map_util\n",
        "\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# What model to download.\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = '/datalab/fine_tuned_model' + '/frozen_inference_graph.pb'\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = os.path.join('/content/datalab', 'label_map.pbtxt')\n",
        "\n",
        "NUM_CLASSES = 37\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.GraphDef()\n",
        "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
        "PATH_TO_TEST_IMAGES_DIR = '/datalab/testsamples/'\n",
        "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.JPG'.format(i)) for i in range(1, 4) ]\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "  with graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "      # Get handles to input and output tensors\n",
        "      ops = tf.get_default_graph().get_operations()\n",
        "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "      tensor_dict = {}\n",
        "      for key in [\n",
        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
        "          'detection_classes', 'detection_masks'\n",
        "      ]:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "              tensor_name)\n",
        "      if 'detection_masks' in tensor_dict:\n",
        "        # The following processing is only for single image\n",
        "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
        "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
        "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
        "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
        "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "        detection_masks_reframed = tf.cast(\n",
        "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "        # Follow the convention by adding back the batch dimension\n",
        "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "            detection_masks_reframed, 0)\n",
        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "      # Run inference\n",
        "      output_dict = sess.run(tensor_dict,\n",
        "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "      output_dict['detection_classes'] = output_dict[\n",
        "          'detection_classes'][0].astype(np.uint8)\n",
        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "      if 'detection_masks' in output_dict:\n",
        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "  return output_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  image = Image.open(image_path)\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "  plt.figure(figsize=IMAGE_SIZE)\n",
        "  plt.imshow(image_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cJTzq7If9lV"
      },
      "source": [
        "## Run Inference on Additional Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kOKL6IwbJwD"
      },
      "source": [
        "%cd /root/models/research/object_detection\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "#if tf.__version__ < '1.4.0':\n",
        "#  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from utils import label_map_util\n",
        "\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# What model to download.\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = '/datalab/fine_tuned_model' + '/frozen_inference_graph.pb'\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = os.path.join('/content/datalab', 'label_map.pbtxt')\n",
        "\n",
        "NUM_CLASSES = 37\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.GraphDef()\n",
        "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
        "PATH_TO_TEST_IMAGES_DIR = '/datalab/images/'\n",
        "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'solar_{}.JPG'.format(i)) for i in range(150, 166) ]\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "  with graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "      # Get handles to input and output tensors\n",
        "      ops = tf.get_default_graph().get_operations()\n",
        "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "      tensor_dict = {}\n",
        "      for key in [\n",
        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
        "          'detection_classes', 'detection_masks'\n",
        "      ]:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "              tensor_name)\n",
        "      if 'detection_masks' in tensor_dict:\n",
        "        # The following processing is only for single image\n",
        "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
        "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
        "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
        "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
        "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "        detection_masks_reframed = tf.cast(\n",
        "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "        # Follow the convention by adding back the batch dimension\n",
        "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "            detection_masks_reframed, 0)\n",
        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "      # Run inference\n",
        "      output_dict = sess.run(tensor_dict,\n",
        "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "      output_dict['detection_classes'] = output_dict[\n",
        "          'detection_classes'][0].astype(np.uint8)\n",
        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "      if 'detection_masks' in output_dict:\n",
        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "  return output_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  image = Image.open(image_path)\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "  plt.figure(figsize=IMAGE_SIZE)\n",
        "  plt.imshow(image_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JCChMpsfOz0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "ca63cae6-f88e-4a49-e0ac-a808d6fc8c9a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "licensees = pd.read_csv(\"/content/sample_data/train_labels.csv\", index_col=0)[1:]\n",
        "licensees.head(10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>xmin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymin</th>\n",
              "      <th>ymax</th>\n",
              "      <th>filename</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Defect</th>\n",
              "      <td>0</td>\n",
              "      <td>115</td>\n",
              "      <td>3351</td>\n",
              "      <td>734</td>\n",
              "      <td>framee (2).jpg</td>\n",
              "      <td>3840</td>\n",
              "      <td>2160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Defect</th>\n",
              "      <td>0</td>\n",
              "      <td>115</td>\n",
              "      <td>3346</td>\n",
              "      <td>725</td>\n",
              "      <td>framee (3).jpg</td>\n",
              "      <td>3840</td>\n",
              "      <td>2160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Defect</th>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>3342</td>\n",
              "      <td>752</td>\n",
              "      <td>framee (4).jpg</td>\n",
              "      <td>3840</td>\n",
              "      <td>2160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Defect</th>\n",
              "      <td>0</td>\n",
              "      <td>115</td>\n",
              "      <td>3351</td>\n",
              "      <td>765</td>\n",
              "      <td>framee (5).jpg</td>\n",
              "      <td>3840</td>\n",
              "      <td>2160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Defect</th>\n",
              "      <td>5</td>\n",
              "      <td>115</td>\n",
              "      <td>3351</td>\n",
              "      <td>761</td>\n",
              "      <td>framee (6).jpg</td>\n",
              "      <td>3840</td>\n",
              "      <td>2160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Defect</th>\n",
              "      <td>0</td>\n",
              "      <td>134</td>\n",
              "      <td>3342</td>\n",
              "      <td>743</td>\n",
              "      <td>framee (7).jpg</td>\n",
              "      <td>3840</td>\n",
              "      <td>2160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Defect</th>\n",
              "      <td>5</td>\n",
              "      <td>134</td>\n",
              "      <td>3360</td>\n",
              "      <td>729</td>\n",
              "      <td>framee (8).jpg</td>\n",
              "      <td>3840</td>\n",
              "      <td>2160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Defect</th>\n",
              "      <td>0</td>\n",
              "      <td>134</td>\n",
              "      <td>3355</td>\n",
              "      <td>761</td>\n",
              "      <td>framee (9).jpg</td>\n",
              "      <td>3840</td>\n",
              "      <td>2160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Defect</th>\n",
              "      <td>5</td>\n",
              "      <td>79</td>\n",
              "      <td>3346</td>\n",
              "      <td>788</td>\n",
              "      <td>framee (1).jpg</td>\n",
              "      <td>3840</td>\n",
              "      <td>2160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Defect</th>\n",
              "      <td>0</td>\n",
              "      <td>165</td>\n",
              "      <td>3369</td>\n",
              "      <td>752</td>\n",
              "      <td>framee (10).jpg</td>\n",
              "      <td>3840</td>\n",
              "      <td>2160</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        xmin  xmax  ymin  ymax         filename  width  height\n",
              "class                                                         \n",
              "Defect     0   115  3351   734   framee (2).jpg   3840    2160\n",
              "Defect     0   115  3346   725   framee (3).jpg   3840    2160\n",
              "Defect     0   129  3342   752   framee (4).jpg   3840    2160\n",
              "Defect     0   115  3351   765   framee (5).jpg   3840    2160\n",
              "Defect     5   115  3351   761   framee (6).jpg   3840    2160\n",
              "Defect     0   134  3342   743   framee (7).jpg   3840    2160\n",
              "Defect     5   134  3360   729   framee (8).jpg   3840    2160\n",
              "Defect     0   134  3355   761   framee (9).jpg   3840    2160\n",
              "Defect     5    79  3346   788   framee (1).jpg   3840    2160\n",
              "Defect     0   165  3369   752  framee (10).jpg   3840    2160"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNdVNfie7Yjt"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats # some useful stuff\n",
        "wine_data = pd.read_csv(\"/content/sample_data/train_labels.csv\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mJCR8VpL7Sx",
        "outputId": "a26c35f1-ebae-426a-9154-d0817a9c3e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "i = int(0)\n",
        "while (i<60):\n",
        "  {\n",
        "    if ('TRUE' == wine_data['class[i]']):\n",
        "      wine_data['class[i]'] = 1\n",
        "    else:\n",
        "      wine_data['class[i]'] = 0\n",
        " \n",
        "  i++;\n",
        "  }"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-3a9e98d9c86b>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    if ('TRUE' == wine_data['class']):\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3-lfTdZUmWc",
        "outputId": "80b661ac-e38f-4a08-a07d-a8d774613855",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS2iqVKA7jY1",
        "outputId": "07d14071-f830-472e-92d9-ce9f15447be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "\n",
        "points = wine_data['class']\n",
        "mu = points.mean()\n",
        "sigma = points.std(ddof=0)\n",
        "print(\"mu: \", mu, \", sigma:\", sigma)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'TRUEDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectTRUETRUETRUETRUETRUETRUETRUETRUETRUETRUETRUETRUETRUETRUETRUEDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefect'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1430\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: complex() arg is a malformed string",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-206f14ca3470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwine_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mu: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", sigma:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11473\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agg_by_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11474\u001b[0m         return self._reduce(\n\u001b[0;32m> 11475\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11476\u001b[0m         )\n\u001b[1;32m  11477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   4247\u001b[0m                 )\n\u001b[1;32m   4248\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4249\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reindex_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    127\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mdtype_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ndim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m                 \u001b[0;31m# e.g. \"foo\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not convert {x} to numeric\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not convert TRUEDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectTRUETRUETRUETRUETRUETRUETRUETRUETRUETRUETRUETRUETRUETRUETRUEDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefectDefect to numeric"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHKaABHp-HP8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}